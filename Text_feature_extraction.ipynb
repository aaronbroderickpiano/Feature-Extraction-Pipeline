{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class TextFeatures:\n",
    "    \n",
    "    def __init__(self, data, data_target):\n",
    "        \n",
    "        self.data = data # x_variables\n",
    "        self.data_target = data_target # y_variable\n",
    "        \n",
    "        # The modeling pipeline\n",
    "        \n",
    "        self.pipeline = Pipeline([\n",
    "                                ('vect', CountVectorizer()), \n",
    "                                ('tfidf', TfidfTransformer()), \n",
    "                                ('clf', SGDClassifier())], \n",
    "                                )\n",
    "        \n",
    "        # The parameters to test by grid search\n",
    "        \n",
    "        self.parameters = {\n",
    "                        'vect__max_df': (0.5, 0.75, 1.0), # If word count percentage is higher than x, remove word.\n",
    "                        'vect__max_features': (None, 5000, 10000, 50000), # Max features to consider.  None = all features\n",
    "                        'vect__ngram_range': ((1, 1), (1, 2)),  # Unigrams or bigrams\n",
    "                        'tfidf__use_idf': (True, False), # Yes/no to use inverse document frequency\n",
    "                        'tfidf__norm': ('l1', 'l2'), # Choose normalization for tfidf\n",
    "                        'clf__alpha': (0.00001, 0.000001), # Toggling the alpha for normalization in the classifier\n",
    "                        'clf__penalty': ('l2', 'elasticnet'), # Norm type for classifier\n",
    "                        'clf__n_iter': (10, 50, 80), # Number of iterations to try. \n",
    "                        }\n",
    "        \n",
    "    def get_best_features(self):\n",
    "        \n",
    "        if __name__ == \"__main__\":\n",
    "            \n",
    "            # Make the grid search.  n_jobs = -1 means use all processors\n",
    "            \n",
    "            grid_search = GridSearchCV(self.pipeline, self.parameters, n_jobs=-1) \n",
    "            \n",
    "            # Fit your data\n",
    "            \n",
    "            grid_search.fit(self.data, self.data_target)\n",
    "            \n",
    "            # This result gives ALL the parameters even if they were not explicitly asked for.\n",
    "            \n",
    "            best_parameters = grid_search.best_estimator_.get_params() \n",
    "            \n",
    "            # Now create a dict of just the parameters you are interested in.\n",
    "            \n",
    "            result = {}\n",
    "            \n",
    "            for param_name in sorted(self.parameters.keys()):\n",
    "                result[param_name] = best_parameters[param_name]\n",
    "                \n",
    "            # Store the accuracy\n",
    "                \n",
    "            result['best_result'] = grid_search.best_score_\n",
    "        \n",
    "        # I outputed this as a dict so it would be easy to insert into a final pipeline.  \n",
    "        # Could be outputed in various formats depending upon the use case.  The actual trained \n",
    "        # model could also be exported as well. \n",
    "            \n",
    "        return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "\n",
    "x = TextFeatures(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 1e-05,\n",
       " 'clf__n_iter': 50,\n",
       " 'clf__penalty': 'l2',\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__max_df': 0.75,\n",
       " 'vect__max_features': 50000,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'best_result': 0.9474912485414235}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_best_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
